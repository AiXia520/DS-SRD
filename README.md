# DS-SRD: A Unified Framework for Structured Representation Distillation

To improve the representation performance of smaller models, representation distillation has been investigated to transfer structured knowledge from a larger model (teacher) to a smaller model (student). Existing works aim to maximize a lower bound on mutual information to transfer global structure knowledge, thus ignoring the local structured semantic transfer from teacher representation. We propose a unified framework for Structured Representation Distillation with the Double Student training mechanism, named DS-SRD, which focuses on transferring global and local structured consistent representation between teacher and student. The motivation is that a good teacher network could construct a well-structured feature space in terms of global and local dependencies. DS-SRD makes the student mimic better structured semantic relations from the teacher, thus improving not only the representation performance but also can easily extend to different distillation tasks: supervised representation distillation and self-supervised representation distillation. In addition, we also design a simple CNN-Transformer structure based on DS-SRD to make the CNN encoder attentive via transformer guidance. Through extensive experiments, we validate the effectiveness of our method compared to various supervised and self-supervised representation distillation baselines. 
